import{_ as e,o as s,c as n,R as a}from"./chunks/framework.zUbWieqp.js";const b=JSON.parse('{"title":"Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？","description":"","frontmatter":{},"headers":[],"relativePath":"guide/Database/etcd/base/20-Kubernetes高级应用.md","filePath":"guide/Database/etcd/base/20-Kubernetes高级应用.md","lastUpdated":1703142476000}'),t={name:"guide/Database/etcd/base/20-Kubernetes高级应用.md"},p=a(`<h1 id="kubernetes高级应用-如何优化业务场景使etcd能支撑上万节点集群" tabindex="-1">Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？ <a class="header-anchor" href="#kubernetes高级应用-如何优化业务场景使etcd能支撑上万节点集群" aria-label="Permalink to &quot;Kubernetes高级应用：如何优化业务场景使etcd能支撑上万节点集群？&quot;">​</a></h1><p>虽然 Kubernetes 社区官网文档目前声称支持最大集群节点数为 5000，但是云厂商已经号称支持 15000 节点的 Kubernetes 集群了，那么为什么一个小小的 etcd 能支撑 15000 节点 Kubernetes 集群呢？</p><h2 id="大集群核心问题分析" tabindex="-1">大集群核心问题分析 <a class="header-anchor" href="#大集群核心问题分析" aria-label="Permalink to &quot;大集群核心问题分析&quot;">​</a></h2><p>在大规模 Kubernetes 集群中会遇到哪些问题呢？</p><p>大规模 Kubernetes 集群的外在表现是节点数成千上万，资源对象数量高达几十万。本质是更频繁地查询、写入更大的资源对象</p><p>==首先是查询相关问题==。在大集群中最重要的就是如何最大程度地减少 expensive request。因为对几十万级别的对象数量来说，按标签、namespace 查询 Pod，获取所有 Node 等场景时，很容易造成 etcd 和 kube-apiserver OOM 和丢包，乃至雪崩等问题发生</p><p>==其次是写入相关问题==。Kubernetes 为了维持上万节点的心跳，会产生大量写请求。而按照我们基础篇介绍的 etcd MVCC、boltdb、线性读等原理，etcd 适用场景是读多写少，大量写请求可能会导致 db size 持续增长、写性能达到瓶颈被限速、影响读性能</p><p>==最后是大资源对象相关问题==。etcd 适合存储较小的 key-value 数据，etcd 本身也做了一系列硬限制，比如 key 的 value 大小默认不能超过 1.5MB</p><h2 id="如何减少-expensive-request" tabindex="-1">如何减少 expensive request <a class="header-anchor" href="#如何减少-expensive-request" aria-label="Permalink to &quot;如何减少 expensive request&quot;">​</a></h2><p>首先是第一个问题，Kubernetes 如何减少 expensive request？</p><p>在这个问题中，我将 Kubernetes 解决此问题的方案拆分成几个核心点和你分析。</p><h3 id="分页" tabindex="-1">分页 <a class="header-anchor" href="#分页" aria-label="Permalink to &quot;分页&quot;">​</a></h3><p>首先 List 资源操作是个基本功能点。各个组件在启动的时候，都不可避免会产生 List 操作，从 etcd 获取集群资源数据，构建初始状态。因此优化的第一步就是要避免一次性读取数十万的资源操作</p><p>解决方案是 Kubernetes List 接口支持分页特性。分页特性依赖底层存储支持，早期的 etcd v2 并未支持分页被饱受诟病，非常容易出现 kube-apiserver 大流量、高负载等问题。在 etcd v3 中，实现了指定返回 Limit 数量的范围查询，因此也赋能 kube-apiserver 对外提供了分页能力</p><p>如下所示，在 List 接口的 ListOption 结构体中，Limit 和 Continue 参数就是为了实现分页特性而增加的</p><p>Limit 表示一次 List 请求最多查询的对象数量，一般为 500。如果实际对象数量大于 Limit，kube-apiserver 则会更新 ListMeta 的 Continue 字段，client 发起的下一个 List 请求带上这个字段就可获取下一批对象数量。直到 kube-apiserver 返回空的 Continue 值，就获取完成了整个对象结果集</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">// ListOptions is the query options to a standard REST </span></span>
<span class="line"><span style="color:#e1e4e8;">list call.</span></span>
<span class="line"><span style="color:#e1e4e8;">type ListOptions struct {</span></span>
<span class="line"><span style="color:#e1e4e8;">   ...</span></span>
<span class="line"><span style="color:#e1e4e8;">   Limit int64 \`json:&quot;limit,omitempty&quot; </span></span>
<span class="line"><span style="color:#e1e4e8;">protobuf:&quot;varint,7,opt,name=limit&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   Continue string \`json:&quot;continue,omitempty&quot; </span></span>
<span class="line"><span style="color:#e1e4e8;">protobuf:&quot;bytes,8,opt,name=continue&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">}</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">// ListOptions is the query options to a standard REST </span></span>
<span class="line"><span style="color:#24292e;">list call.</span></span>
<span class="line"><span style="color:#24292e;">type ListOptions struct {</span></span>
<span class="line"><span style="color:#24292e;">   ...</span></span>
<span class="line"><span style="color:#24292e;">   Limit int64 \`json:&quot;limit,omitempty&quot; </span></span>
<span class="line"><span style="color:#24292e;">protobuf:&quot;varint,7,opt,name=limit&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   Continue string \`json:&quot;continue,omitempty&quot; </span></span>
<span class="line"><span style="color:#24292e;">protobuf:&quot;bytes,8,opt,name=continue&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">}</span></span></code></pre></div><p>了解完 kube-apiserver 的分页特性后，我们接着往下看 Continue 字段具体含义，以及它是如何影响 etcd 查询结果的</p><p>我们知道 etcd 分页是通过范围查询和 Limit 实现，ListOption 中的 Limit 对应 etcd 查询接口中的 Limit 参数。你可以大胆猜测下，Continue 字段是不是跟查询的范围起始 key 相关呢？</p><p>Continue 字段的确包含查询范围的起始 key，它本质上是个结构体，还包含 APIVersion 和 ResourceVersion。你之所以看到的是一个奇怪字符串，那是因为 kube-apiserver 使用 base64 库对其进行了 URL 编码，下面是它的原始结构体</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">type continueToken struct {</span></span>
<span class="line"><span style="color:#e1e4e8;">   APIVersion      string \`json:&quot;v&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   ResourceVersion int64  \`json:&quot;rv&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   StartKey        string \`json:&quot;start&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">}</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">type continueToken struct {</span></span>
<span class="line"><span style="color:#24292e;">   APIVersion      string \`json:&quot;v&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   ResourceVersion int64  \`json:&quot;rv&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   StartKey        string \`json:&quot;start&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">}</span></span></code></pre></div><p>当 kube-apiserver 收到带 Continue 的分页查询时，解析 Continue，获取 StartKey、ResourceVersion，etcd 查询 Range 接口指定 startKey，增加 clienv3.WithRange、clientv3.WithLimit、clientv3.WithRev 即可</p><p>当你通过分页多次查询 Kubernetes 资源对象，得到的最终结果集合与不带 Limit 查询结果是一致的吗？kube-apiserver 是如何保证分页查询的一致性呢？ 这个问题我把它作为了思考题，我们一起讨论</p><h2 id="资源按-namespace-拆分" tabindex="-1">资源按 namespace 拆分 <a class="header-anchor" href="#资源按-namespace-拆分" aria-label="Permalink to &quot;资源按 namespace 拆分&quot;">​</a></h2><p>通过分页特性提供机制避免一次拉取大量资源对象后，接下来就是业务最佳实践上要避免同 namespace 存储大量资源，尽量将资源对象拆分到不同 namespace 下</p><p>为什么拆分到不同 namespace 下有助于提升性能呢?</p><p>正如我在19中所介绍的，Kubernetes 资源对象存储在 etcd 中的 key 前缀包含 namespace，因此它相当于是个高效的索引字段。etcd treeIndex 模块从 B-tree 中匹配前缀时，可快速过滤出符合条件的 key-value 数据</p><p>Kubernetes 社区承诺<a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md" target="_blank" rel="noreferrer">SLO</a>达标的前提是，你在使用 Kubernetes 集群过程中必须合理配置集群和使用扩展特性，并遵循<a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md" target="_blank" rel="noreferrer">一系列条件限制</a>（比如同 namespace 下的 Service 数量不超过 5000 个）</p><h2 id="informer-机制" tabindex="-1">Informer 机制 <a class="header-anchor" href="#informer-机制" aria-label="Permalink to &quot;Informer 机制&quot;">​</a></h2><p>各组件启动发起一轮 List 操作加载完初始状态数据后，就进入了控制器的一致性协调逻辑。在一致性协调逻辑中，在 19 讲 Kubernetes 基础篇中，我和你介绍了 Kubernetes 使用的是 Watch 特性来获取数据变化通知，而不是 List 定时轮询，这也是减少 List 操作一大核心策略</p><p>Kubernetes 社区在 client-go 项目中提供了一个通用的 Informer 组件来负责 client 与 kube-apiserver 进行资源和事件同步，显著降低了开发者使用 Kubernetes API、开发高性能 Kubernetes 扩展组件的复杂度</p><p>Informer 机制的 Reflector 封装了 Watch、List 操作，结合本地 Cache、Indexer，实现了控制器加载完初始状态数据后，接下来的其他操作都只需要从本地缓存读取，极大降低了 kube-apiserver 和 etcd 的压力</p><p>下面是 Kubernetes 社区给出的一个控制器使用 Informer 机制的架构图。黄色部分是控制器相关基础组件，蓝色部分是 client-go 的 Informer 机制的组件，它由 Reflector、Queue、Informer、Indexer、Thread safe store(Local Cache) 组成</p><p><img src="https://nnaigos.oss-cn-hangzhou.aliyuncs.com/imgs/202312211506333.jpg" alt=""></p><p>Informer 机制的基本工作流程如下：</p><ul><li>client 启动或与 kube-apiserver 出现连接中断再次 Watch 时，报&quot;too old resource version&quot;等错误后，通过 Reflector 组件的 List 操作，从 kube-apiserver 获取初始状态数据，随后通过 Watch 机制实时监听数据变化</li><li>收到事件后添加到 Delta FIFO 队列，由 Informer 组件进行处理</li><li>Informer 将 delta FIFO 队列中的事件转发给 Indexer 组件，Indexer 组件将事件持久化存储在本地的缓存中</li><li>控制器开发者可通过 Informer 组件注册 Add、Update、Delete 事件的回调函数。Informer 组件收到事件后会回调业务函数，比如典型的控制器使用场景，一般是将各个事件添加到 WorkQueue 中，控制器的各个协调 goroutine 从队列取出消息，解析 key，通过 key 从 Informer 机制维护的本地 Cache 中读取数据</li></ul><p>通过以上流程分析，你可以发现除了启动、连接中断等场景才会触发 List 操作，其他时候都是从本地 Cache 读取</p><p>那连接中断等场景为什么触发 client List 操作呢？</p><h2 id="watch-bookmark-机制" tabindex="-1">Watch bookmark 机制 <a class="header-anchor" href="#watch-bookmark-机制" aria-label="Permalink to &quot;Watch bookmark 机制&quot;">​</a></h2><p>要搞懂这个问题，你得了解 kube-apiserver Watch 特性的原理</p><p>接下来我就和你介绍下 Kubernetes 的 Watch 特性。我们知道 Kubernetes 通过全局递增的 Resource Version 来实现增量数据同步逻辑，尽量避免连接中断等异常场景下 client 发起全量 List 同步操作</p><p>那么在什么场景下会触发全量 List 同步操作呢？这就取决于 client 请求的 Resource Version 以及 kube-apiserver 中是否还保存了相关的历史版本数据</p><p>在08Watch 特性中，我和你提到实现历史版本数据存储两大核心机制，滑动窗口和 MVCC。与 etcd v3 使用 MVCC 机制不一样的是，Kubernetes 采用的是滑动窗口机制</p><p>kube-apiserver 的滑动窗口机制是如何实现的呢?</p><p>它通过为每个类型资源（Pod,Node 等）维护一个 cyclic buffer，来存储最近的一系列变更事件实现</p><p>下面 Kubernetes 核心的 watchCache 结构体中的 cache 数组、startIndex、endIndex 就是用来实现 cyclic buffer 的。滑动窗口中的第一个元素就是 cache[startIndex%capacity]，最后一个元素则是 cache[endIndex%capacity]</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">// watchCache is a &quot;sliding window&quot; (with a limited capacity) of objects</span></span>
<span class="line"><span style="color:#e1e4e8;">// observed from a watch.</span></span>
<span class="line"><span style="color:#e1e4e8;">type watchCache struct {</span></span>
<span class="line"><span style="color:#e1e4e8;">   sync.RWMutex</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">   // Condition on which lists are waiting for the fresh enough</span></span>
<span class="line"><span style="color:#e1e4e8;">   // resource version.</span></span>
<span class="line"><span style="color:#e1e4e8;">   cond *sync.Cond</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">   // Maximum size of history window.</span></span>
<span class="line"><span style="color:#e1e4e8;">   capacity int</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">   // upper bound of capacity since event cache has a dynamic size.</span></span>
<span class="line"><span style="color:#e1e4e8;">   upperBoundCapacity int</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">   // lower bound of capacity since event cache has a dynamic size.</span></span>
<span class="line"><span style="color:#e1e4e8;">   lowerBoundCapacity int</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">   // cache is used a cyclic buffer - its first element (with the smallest</span></span>
<span class="line"><span style="color:#e1e4e8;">   // resourceVersion) is defined by startIndex, its last element is defined</span></span>
<span class="line"><span style="color:#e1e4e8;">   // by endIndex (if cache is full it will be startIndex + capacity).</span></span>
<span class="line"><span style="color:#e1e4e8;">   // Both startIndex and endIndex can be greater than buffer capacity -</span></span>
<span class="line"><span style="color:#e1e4e8;">   // you should always apply modulo capacity to get an index in cache array.</span></span>
<span class="line"><span style="color:#e1e4e8;">   cache      []*watchCacheEvent</span></span>
<span class="line"><span style="color:#e1e4e8;">   startIndex int</span></span>
<span class="line"><span style="color:#e1e4e8;">   endIndex   int</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">   // store will effectively support LIST operation from the &quot;end of cache</span></span>
<span class="line"><span style="color:#e1e4e8;">   // history&quot; i.e. from the moment just after the newest cached watched event.</span></span>
<span class="line"><span style="color:#e1e4e8;">   // It is necessary to effectively allow clients to start watching at now.</span></span>
<span class="line"><span style="color:#e1e4e8;">   // NOTE: We assume that &lt;store&gt; is thread-safe.</span></span>
<span class="line"><span style="color:#e1e4e8;">   store cache.Indexer</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">   // ResourceVersion up to which the watchCache is propagated.</span></span>
<span class="line"><span style="color:#e1e4e8;">   resourceVersion uint64</span></span>
<span class="line"><span style="color:#e1e4e8;">}</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">// watchCache is a &quot;sliding window&quot; (with a limited capacity) of objects</span></span>
<span class="line"><span style="color:#24292e;">// observed from a watch.</span></span>
<span class="line"><span style="color:#24292e;">type watchCache struct {</span></span>
<span class="line"><span style="color:#24292e;">   sync.RWMutex</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">   // Condition on which lists are waiting for the fresh enough</span></span>
<span class="line"><span style="color:#24292e;">   // resource version.</span></span>
<span class="line"><span style="color:#24292e;">   cond *sync.Cond</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">   // Maximum size of history window.</span></span>
<span class="line"><span style="color:#24292e;">   capacity int</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">   // upper bound of capacity since event cache has a dynamic size.</span></span>
<span class="line"><span style="color:#24292e;">   upperBoundCapacity int</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">   // lower bound of capacity since event cache has a dynamic size.</span></span>
<span class="line"><span style="color:#24292e;">   lowerBoundCapacity int</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">   // cache is used a cyclic buffer - its first element (with the smallest</span></span>
<span class="line"><span style="color:#24292e;">   // resourceVersion) is defined by startIndex, its last element is defined</span></span>
<span class="line"><span style="color:#24292e;">   // by endIndex (if cache is full it will be startIndex + capacity).</span></span>
<span class="line"><span style="color:#24292e;">   // Both startIndex and endIndex can be greater than buffer capacity -</span></span>
<span class="line"><span style="color:#24292e;">   // you should always apply modulo capacity to get an index in cache array.</span></span>
<span class="line"><span style="color:#24292e;">   cache      []*watchCacheEvent</span></span>
<span class="line"><span style="color:#24292e;">   startIndex int</span></span>
<span class="line"><span style="color:#24292e;">   endIndex   int</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">   // store will effectively support LIST operation from the &quot;end of cache</span></span>
<span class="line"><span style="color:#24292e;">   // history&quot; i.e. from the moment just after the newest cached watched event.</span></span>
<span class="line"><span style="color:#24292e;">   // It is necessary to effectively allow clients to start watching at now.</span></span>
<span class="line"><span style="color:#24292e;">   // NOTE: We assume that &lt;store&gt; is thread-safe.</span></span>
<span class="line"><span style="color:#24292e;">   store cache.Indexer</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">   // ResourceVersion up to which the watchCache is propagated.</span></span>
<span class="line"><span style="color:#24292e;">   resourceVersion uint64</span></span>
<span class="line"><span style="color:#24292e;">}</span></span></code></pre></div><p>下面我以 Pod 资源的历史事件滑动窗口为例，和你聊聊它在什么场景可能会触发 client 全量 List 同步操作</p><p>如下图所示，kube-apiserver 启动后，通过 List 机制，加载初始 Pod 状态数据，随后通过 Watch 机制监听最新 Pod 数据变化。当你不断对 Pod 资源进行增加、删除、修改后，携带新 Resource Version（简称 RV）的 Pod 事件就会不断被加入到 cyclic buffer。假设 cyclic buffer 容量为 100，RV1 是最小的一个 Watch 事件的 Resource Version，RV 100 是最大的一个 Watch 事件的 Resource Version</p><p>当版本号为 RV101 的 Pod 事件到达时，RV1 就会被淘汰，kube-apiserver 维护的 Pod 最小版本号就变成了 RV2。然而在 Kubernetes 集群中，不少组件都只关心 cyclic buffer 中与自己相关的事件。</p><p><img src="https://nnaigos.oss-cn-hangzhou.aliyuncs.com/imgs/202312211506725.jpg" alt=""></p><p>比如图中的 kubelet 只关注运行在自己节点上的 Pod，假设只有 RV1 是它关心的 Pod 事件版本号，在未实现 Watch bookmark 特性之前，其他 RV2 到 RV101 的事件是不会推送给它的，因此它内存中维护的 Resource Version 依然是 RV1</p><p>若此 kubelet 随后与 kube-apiserver 连接出现异常，它将使用版本号 RV1 发起 Watch 重连操作。但是 kube-apsierver cyclic buffer 中的 Pod 最小版本号已是 RV2，因此会返回&quot;too old resource version&quot;错误给 client，client 只能发起 List 操作，在获取到最新版本号后，才能重新进入监听逻辑</p><p>那么我们能否定时将最新的版本号推送给各个 client 来解决以上问题呢?</p><p>是的，这就是 Kubernetes 的 Watch bookmark 机制核心思想。即使队列中无 client 关注的更新事件，Informer 机制的 Reflector 组件中 Resource Version 也需要更新</p><p>Watch bookmark 机制通过新增一个 bookmark 类型的事件来实现的。kube-apiserver 会通过定时器将各类型资源最新的 Resource Version 推送给 kubelet 等 client，在 client 与 kube-apiserver 网络异常重连等场景，大大降低了 client 重建 Watch 的开销，减少了 relist expensive request</p><h2 id="更高效的-watch-恢复机" tabindex="-1">更高效的 Watch 恢复机 <a class="header-anchor" href="#更高效的-watch-恢复机" aria-label="Permalink to &quot;更高效的 Watch 恢复机&quot;">​</a></h2><p>虽然 Kubernetes 社区通过 Watch bookmark 机制缓解了 client 与 kube-apiserver 重连等场景下可能导致的 relist expensive request 操作，然而在 kube-apiserver 重启、滚动更新时，它依然还是有可能导致大量的 relist 操作，这是为什么呢？ 如何进一步减少 kube-apiserver 重启场景下的 List 操作呢？</p><p>如下图所示，在 kube-apiserver 重启后，kubelet 等 client 会立刻带上 Resource Version 发起重建 Watch 的请求。问题就在 kube-apiserver 重启后，watchCache 中的 cyclic buffer 是空的，此时 watchCache 中的最小 Resource Version(listResourceVersion) 是 etcd 的最新全局版本号，也就是图中的 RV200</p><p><img src="https://nnaigos.oss-cn-hangzhou.aliyuncs.com/imgs/202312211506752.jpg" alt=""></p><p>在不少场景下，client 请求重建 Watch 的 Resource Version 是可能小于 listResourceVersion 的</p><p>比如在上面的这个案例图中，集群内 Pod 稳定运行未发生变化，kubelet 假设收到了最新的 RV100 事件。然而这个集群其他资源如 ConfigMap，被管理员不断的修改，它就会导致导致 etcd 版本号新增，ConfigMap 滑动窗口也会不断存储变更事件，从图中可以看到，它记录最大版本号为 RV200</p><p>因此 kube-apiserver 重启后，client 请求重建 Pod Watch 的 Resource Version 是 RV100，而 Pod watchCache 中的滑动窗口最小 Resource Version 是 RV200。很显然，RV100 不在 Pod watchCache 所维护的滑动窗口中，kube-apiserver 就会返回&quot;too old resource version&quot;错误给 client，client 只能发起 relist expensive request 操作同步最新数据</p><p>为了进一步降低 kube-apiserver 重启对 client Watch 中断的影响，Kubernetes 在 1.20 版本中又进一步实现了<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1904-efficient-watch-resumption" target="_blank" rel="noreferrer">更高效的 Watch 恢复机制</a>。它通过 etcd Watch 机制的 Notify 特性，实现了将 etcd 最新的版本号定时推送给 kube-apiserver。kube-apiserver 在将其转换成 ResourceVersion 后，再通过 bookmark 机制推送给 client，避免了 kube-apiserver 重启后 client 可能发起的 List 操作</p><h2 id="如何控制-db-size" tabindex="-1">如何控制 db size <a class="header-anchor" href="#如何控制-db-size" aria-label="Permalink to &quot;如何控制 db size&quot;">​</a></h2><p>分析完 Kubernetes 如何减少 expensive request，我们再看看 Kubernetes 是如何控制 db size 的</p><p>首先，我们知道 Kubernetes 的 kubelet 组件会每隔 10 秒上报一次心跳给 kube-apiserver。</p><p>其次，Node 资源对象因为包含若干个镜像、数据卷等信息，导致 Node 资源对象会较大，一次心跳消息可能高达 15KB 以上</p><p>最后，etcd 是基于 COW(Copy-on-write) 机制实现的 MVCC 数据库，每次修改都会产生新的 key-value，若大量写入会导致 db size 持续增长</p><p>早期 Kubernetes 集群由于以上原因，当节点数成千上万时，kubelet 产生的大量写请求就较容易造成 db 大小达到配额，无法写入</p><p>那么如何解决呢？</p><p>本质上还是 Node 资源对象大的问题。实际上我们需要更新的仅仅是 Node 资源对象的心跳状态，而在 etcd 中我们存储的是整个 Node 资源对象，并未将心跳状态拆分出来</p><p>因此 Kuberentes 的解决方案就是将 Node 资源进行拆分，把心跳状态信息从 Node 对象中剥离出来，通过下面的 Lease 对象来描述它。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">// Lease defines a lease concept.</span></span>
<span class="line"><span style="color:#e1e4e8;">type Lease struct {</span></span>
<span class="line"><span style="color:#e1e4e8;">   metav1.TypeMeta \`json:&quot;,inline&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   metav1.ObjectMeta \`json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   Spec LeaseSpec \`json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">}</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">// LeaseSpec is a specification of a Lease.</span></span>
<span class="line"><span style="color:#e1e4e8;">type LeaseSpec struct {</span></span>
<span class="line"><span style="color:#e1e4e8;">   HolderIdentity *string \`json:&quot;holderIdentity,omitempty&quot; protobuf:&quot;bytes,1,opt,name=holderIdentity&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   LeaseDurationSeconds *int32 \`json:&quot;leaseDurationSeconds,omitempty&quot; protobuf:&quot;varint,2,opt,name=leaseDurationSeconds&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   AcquireTime *metav1.MicroTime \`json:&quot;acquireTime,omitempty&quot; protobuf:&quot;bytes,3,opt,name=acquireTime&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   RenewTime *metav1.MicroTime \`json:&quot;renewTime,omitempty&quot; protobuf:&quot;bytes,4,opt,name=renewTime&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">   LeaseTransitions *int32 \`json:&quot;leaseTransitions,omitempty&quot; protobuf:&quot;varint,5,opt,name=leaseTransitions&quot;\`</span></span>
<span class="line"><span style="color:#e1e4e8;">}</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">// Lease defines a lease concept.</span></span>
<span class="line"><span style="color:#24292e;">type Lease struct {</span></span>
<span class="line"><span style="color:#24292e;">   metav1.TypeMeta \`json:&quot;,inline&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   metav1.ObjectMeta \`json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   Spec LeaseSpec \`json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">}</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">// LeaseSpec is a specification of a Lease.</span></span>
<span class="line"><span style="color:#24292e;">type LeaseSpec struct {</span></span>
<span class="line"><span style="color:#24292e;">   HolderIdentity *string \`json:&quot;holderIdentity,omitempty&quot; protobuf:&quot;bytes,1,opt,name=holderIdentity&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   LeaseDurationSeconds *int32 \`json:&quot;leaseDurationSeconds,omitempty&quot; protobuf:&quot;varint,2,opt,name=leaseDurationSeconds&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   AcquireTime *metav1.MicroTime \`json:&quot;acquireTime,omitempty&quot; protobuf:&quot;bytes,3,opt,name=acquireTime&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   RenewTime *metav1.MicroTime \`json:&quot;renewTime,omitempty&quot; protobuf:&quot;bytes,4,opt,name=renewTime&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">   LeaseTransitions *int32 \`json:&quot;leaseTransitions,omitempty&quot; protobuf:&quot;varint,5,opt,name=leaseTransitions&quot;\`</span></span>
<span class="line"><span style="color:#24292e;">}</span></span></code></pre></div><p>因为 Lease 对象非常小，更新的代价远小于 Node 对象，所以这样显著降低了 kube-apiserver 的 CPU 开销、etcd db size，Kubernetes 1.14 版本后已经默认启用 Node 心跳切换到 Lease API</p><h2 id="如何优化-key-value-大小" tabindex="-1">如何优化 key-value 大小 <a class="header-anchor" href="#如何优化-key-value-大小" aria-label="Permalink to &quot;如何优化 key-value 大小&quot;">​</a></h2><p>最后，我们再看看 Kubernetes 是如何解决 etcd key-value 大小限制的</p><p>在成千上万个节点的集群中，一个服务可能背后有上万个 Pod。而服务对应的 Endpoints 资源含有大量的独立的 endpoints 信息，这会导致 Endpoints 资源大小达到 etcd 的 value 大小限制，etcd 拒绝更新</p><p>另外，kube-proxy 等组件会实时监听 Endpoints 资源，一个 endpoint 变化就会产生较大的流量，导致 kube-apiserver 等组件流量超大、出现一系列性能瓶颈</p><p>如何解决以上 Endpoints 资源过大的问题呢？</p><p>答案依然是拆分、化大为小。Kubernetes 社区设计了 EndpointSlice 概念，每个 EndpointSlice 最大支持保存 100 个 endpoints，成功解决了 key-value 过大、变更同步导致流量超大等一系列瓶颈</p><h2 id="etcd-优化" tabindex="-1">etcd 优化 <a class="header-anchor" href="#etcd-优化" aria-label="Permalink to &quot;etcd 优化&quot;">​</a></h2><p>Kubernetes 社区在解决大集群的挑战的同时，etcd 社区也在不断优化、新增特性，提升 etcd 在 Kubernetes 场景下的稳定性和性能。这里我简单列举两个，一个是 etcd 并发读特性，一个是 Watch 特性的 Notify 机制</p><h2 id="并发读特性" tabindex="-1">并发读特性 <a class="header-anchor" href="#并发读特性" aria-label="Permalink to &quot;并发读特性&quot;">​</a></h2><p>通过以上介绍的各种机制、策略，虽然 Kubernetes 能大大缓解 expensive read request 问题，但是它并不是从本质上来解决问题的</p><p>为什么 etcd 无法支持大量的 read expensive request 呢？</p><p>除了我们一直强调的容易导致 OOM、大流量导致丢包外，etcd 根本性瓶颈是在 etcd 3.4 版本之前，expensive read request 会长时间持有 MVCC 模块的 buffer 读锁 RLock。而写请求执行完后，需升级锁至 Lock，expensive request 导致写事务阻塞在升级锁过程中，最终导致写请求超时</p><p>为了解决此问题，etcd 3.4 版本实现了并发读特性。核心解决方案是去掉了读写锁，每个读事务拥有一个 buffer。在收到读请求创建读事务对象时，全量拷贝写事务维护的 buffer 到读事务 buffer 中</p><p>通过并发读特性，显著降低了 List Pod 和 CRD 等 expensive read request 对写性能的影响，延时不再突增、抖动。</p><h2 id="改善-watch-notify-机制" tabindex="-1">改善 Watch Notify 机制 <a class="header-anchor" href="#改善-watch-notify-机制" aria-label="Permalink to &quot;改善 Watch Notify 机制&quot;">​</a></h2><p>为了配合 Kubernetes 社区实现更高效的 Watch 恢复机制，etcd 改善了 Watch Notify 机制，早期 Notify 消息发送间隔是固定的 10 分钟</p><p>在 etcd 3.4.11 版本中，新增了 --experimental-watch-progress-notify-interval 参数使 Notify 间隔时间可配置，最小支持为 100ms，满足了 Kubernetes 业务场景的诉求</p><p>最后，你要注意的是，默认通过 clientv3 Watch API 创建的 watcher 是不会开启此特性的。你需要创建 Watcher 的时候，设置 clientv3.WithProgressNotify 选项，这样 etcd server 就会定时发送提醒消息给 client，消息中就会携带 etcd 当前最新的全局版本号</p>`,93),o=[p];function l(c,i,r,u,d,y){return s(),n("div",null,o)}const m=e(t,[["render",l]]);export{b as __pageData,m as default};
